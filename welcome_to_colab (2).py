# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

# StudyMate - AI-Powered PDF-Based Academic Assistant
# Designed for Google Colab with IBM Granite Model

# Install required packages
!pip install pymupdf sentence-transformers faiss-cpu gradio transformers torch accelerate

import os
import fitz  # PyMuPDF
import faiss
import numpy as np
import gradio as gr
import requests
import json
from sentence_transformers import SentenceTransformer
from typing import List, Tuple, Dict
import re
from datetime import datetime

class StudyMateApp:
    def __init__(self):
        """Initialize StudyMate with necessary components"""
        print("ðŸš€ Initializing StudyMate...")

        # IBM Granite API configuration
        self.api_key = "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"
        self.model_name = "ibm-granite/granite-3.3-2b-instruct"
        self.api_url = f"https://api-inference.huggingface.co/models/{self.model_name}"

        # Initialize embedding model
        print("ðŸ“š Loading sentence transformer...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

        # Storage for documents and embeddings
        self.document_chunks = []
        self.chunk_metadata = []
        self.faiss_index = None
        self.embeddings = None

        print("âœ… StudyMate initialized successfully!")

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF using PyMuPDF"""
        try:
            doc = fitz.open(pdf_path)
            text = ""

            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                text += f"\n--- Page {page_num + 1} ---\n{page_text}"

            doc.close()
            return text
        except Exception as e:
            return f"Error extracting text from PDF: {str(e)}"

    def preprocess_text(self, text: str) -> str:
        """Clean and preprocess extracted text"""
        # Remove excessive whitespace
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r' +', ' ', text)

        # Remove special characters that might interfere
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)]', ' ', text)

        return text.strip()

    def create_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Tuple[str, Dict]]:
        """Split text into overlapping chunks with metadata"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk_text = ' '.join(chunk_words)

            # Extract page number if available
            page_match = re.search(r'--- Page (\d+) ---', chunk_text)
            page_num = int(page_match.group(1)) if page_match else 0

            metadata = {
                'chunk_id': len(chunks),
                'page_number': page_num,
                'word_count': len(chunk_words),
                'start_index': i
            }

            chunks.append((chunk_text, metadata))

        return chunks

    def create_faiss_index(self, texts: List[str]) -> faiss.IndexFlatIP:
        """Create FAISS index from text embeddings"""
        print("ðŸ” Creating embeddings...")
        embeddings = self.embedding_model.encode(texts, convert_to_tensor=False)
        embeddings = np.array(embeddings).astype('float32')

        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)

        # Create FAISS index
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
        index.add(embeddings)

        self.embeddings = embeddings
        return index

    def search_similar_chunks(self, query: str, top_k: int = 3) -> List[Tuple[str, float, Dict]]:
        """Search for similar chunks using FAISS"""
        if self.faiss_index is None:
            return []

        # Encode query
        query_embedding = self.embedding_model.encode([query], convert_to_tensor=False)
        query_embedding = np.array(query_embedding).astype('float32')
        faiss.normalize_L2(query_embedding)

        # Search
        scores, indices = self.faiss_index.search(query_embedding, top_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.document_chunks):
                chunk_text = self.document_chunks[idx]
                metadata = self.chunk_metadata[idx]
                results.append((chunk_text, float(score), metadata))

        return results

    def query_granite_model(self, prompt: str, max_tokens: int = 512) -> str:
        """Query IBM Granite model via Hugging Face API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_tokens,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "return_full_text": False
            }
        }

        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()

            result = response.json()

            if isinstance(result, list) and len(result) > 0:
                return result[0].get('generated_text', 'No response generated.')
            elif isinstance(result, dict):
                return result.get('generated_text', 'No response generated.')
            else:
                return 'Unexpected response format from API.'

        except requests.exceptions.RequestException as e:
            return f"API Error: {str(e)}"
        except Exception as e:
            return f"Error processing response: {str(e)}"

    def generate_answer(self, question: str, context_chunks: List[str]) -> str:
        """Generate answer using retrieved context and Granite model"""
        # Combine context chunks
        context = "\n\n".join(context_chunks[:3])  # Limit context to avoid token limits

        # Create prompt for Granite model
        prompt = f"""Based on the following context from academic materials, provide a comprehensive and accurate answer to the question. If the context doesn't contain enough information to answer the question, please say so.

Context:
{context}

Question: {question}

Answer:"""

        return self.query_granite_model(prompt)

    def process_pdf(self, pdf_file) -> str:
        """Process uploaded PDF file"""
        try:
            if pdf_file is None:
                return "âŒ Please upload a PDF file first."

            print(f"ðŸ“„ Processing PDF: {pdf_file.name}")

            # Extract text
            text = self.extract_text_from_pdf(pdf_file.name)
            if text.startswith("Error"):
                return f"âŒ {text}"

            # Preprocess text
            cleaned_text = self.preprocess_text(text)

            # Create chunks
            chunks_with_metadata = self.create_chunks(cleaned_text)
            self.document_chunks = [chunk[0] for chunk in chunks_with_metadata]
            self.chunk_metadata = [chunk[1] for chunk in chunks_with_metadata]

            # Create FAISS index
            self.faiss_index = self.create_faiss_index(self.document_chunks)

            return f"âœ… PDF processed successfully!\nðŸ“Š Created {len(self.document_chunks)} chunks for analysis."

        except Exception as e:
            return f"âŒ Error processing PDF: {str(e)}"

    def answer_question(self, question: str) -> str:
        """Answer question based on processed PDF"""
        try:
            if not question.strip():
                return "âŒ Please enter a question."

            if self.faiss_index is None:
                return "âŒ Please upload and process a PDF first."

            print(f"ðŸ” Searching for: {question}")

            # Search for relevant chunks
            similar_chunks = self.search_similar_chunks(question, top_k=5)

            if not similar_chunks:
                return "âŒ No relevant content found in the document."

            # Extract chunk texts and metadata
            context_chunks = [chunk[0] for chunk in similar_chunks]

            # Generate answer
            print("ðŸ¤– Generating answer with Granite model...")
            answer = self.generate_answer(question, context_chunks)

            # Format response with references
            response = f"**Answer:**\n{answer}\n\n"
            response += "**Sources:**\n"

            for i, (chunk, score, metadata) in enumerate(similar_chunks[:3]):
                page_info = f"Page {metadata['page_number']}" if metadata['page_number'] > 0 else "Document"
                response += f"â€¢ {page_info} (Relevance: {score:.3f})\n"

            return response

        except Exception as e:
            return f"âŒ Error answering question: {str(e)}"

    def create_interface(self):
        """Create Gradio interface"""
        with gr.Blocks(
            title="StudyMate - AI-Powered PDF Academic Assistant",
            theme=gr.themes.Soft()
        ) as interface:

            # Header
            gr.Markdown("""
            # ðŸ“š StudyMate - AI-Powered PDF Academic Assistant

            Upload your academic PDFs (textbooks, lecture notes, research papers) and ask questions in natural language.
            StudyMate uses IBM Granite AI model to provide contextual answers based on your study materials.

            **Features:**
            - ðŸ“„ Smart PDF text extraction
            - ðŸ” Semantic search with FAISS
            - ðŸ¤– AI-powered answers using IBM Granite model
            - ðŸ“– Source references for reliability
            """)

            with gr.Row():
                with gr.Column(scale=1):
                    # PDF Upload Section
                    gr.Markdown("### 1ï¸âƒ£ Upload Your PDF")
                    pdf_input = gr.File(
                        label="Choose PDF File",
                        file_types=[".pdf"],
                        type="filepath"
                    )

                    process_btn = gr.Button(
                        "ðŸ“„ Process PDF",
                        variant="primary",
                        size="lg"
                    )

                    process_output = gr.Textbox(
                        label="Processing Status",
                        lines=3,
                        interactive=False
                    )

                with gr.Column(scale=1):
                    # Q&A Section
                    gr.Markdown("### 2ï¸âƒ£ Ask Questions")
                    question_input = gr.Textbox(
                        label="Your Question",
                        placeholder="e.g., What is the main concept of machine learning?",
                        lines=2
                    )

                    ask_btn = gr.Button(
                        "ðŸ¤– Get Answer",
                        variant="secondary",
                        size="lg"
                    )

                    answer_output = gr.Textbox(
                        label="StudyMate's Answer",
                        lines=10,
                        interactive=False
                    )

            # Example questions
            gr.Markdown("""
            ### ðŸ’¡ Example Questions You Can Ask:
            - "What are the key principles mentioned in Chapter 3?"
            - "Explain the methodology described in this paper"
            - "What are the main findings or conclusions?"
            - "Define the term mentioned on page 15"
            - "Summarize the introduction section"
            """)

            # Event handlers
            process_btn.click(
                fn=self.process_pdf,
                inputs=[pdf_input],
                outputs=[process_output]
            )

            ask_btn.click(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

            # Allow Enter key to submit question
            question_input.submit(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

        return interface

# Initialize and launch StudyMate
def launch_studymate():
    """Launch StudyMate application"""
    print("ðŸŽ“ Starting StudyMate - AI-Powered PDF Academic Assistant")
    print("=" * 60)

    # Create StudyMate instance
    studymate = StudyMateApp()

    # Create and launch interface
    interface = studymate.create_interface()

    # Launch with public sharing enabled for Colab
    interface.launch(
        share=True,  # Creates public link for Colab
        debug=True,
        height=800,
        show_error=True
    )

# Run the application
if __name__ == "__main__":
    launch_studymate()

# StudyMate - AI-Powered PDF-Based Academic Assistant
# Designed for Google Colab with IBM Granite Model

# Install required packages
!pip install pymupdf sentence-transformers faiss-cpu gradio transformers torch accelerate

import os
import fitz  # PyMuPDF
import faiss
import numpy as np
import gradio as gr
import requests
import json
from sentence_transformers import SentenceTransformer
from typing import List, Tuple, Dict
import re
from datetime import datetime

class StudyMateApp:
    def __init__(self):
        """Initialize StudyMate with necessary components"""
        print("ðŸš€ Initializing StudyMate...")

        # IBM Granite API configuration
        self.api_key = "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"
        self.model_name = "ibm-granite/granite-3.3-2b-instruct"
        self.api_url = f"https://api-inference.huggingface.co/models/{self.model_name}"

        # Initialize embedding model
        print("ðŸ“š Loading sentence transformer...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

        # Storage for documents and embeddings
        self.document_chunks = []
        self.chunk_metadata = []
        self.faiss_index = None
        self.embeddings = None

        print("âœ… StudyMate initialized successfully!")

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF using PyMuPDF"""
        try:
            doc = fitz.open(pdf_path)
            text = ""

            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                text += f"\n--- Page {page_num + 1} ---\n{page_text}"

            doc.close()
            return text
        except Exception as e:
            return f"Error extracting text from PDF: {str(e)}"

    def preprocess_text(self, text: str) -> str:
        """Clean and preprocess extracted text"""
        # Remove excessive whitespace
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r' +', ' ', text)

        # Remove special characters that might interfere
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)]', ' ', text)

        return text.strip()

    def create_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Tuple[str, Dict]]:
        """Split text into overlapping chunks with metadata"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk_text = ' '.join(chunk_words)

            # Extract page number if available
            page_match = re.search(r'--- Page (\d+) ---', chunk_text)
            page_num = int(page_match.group(1)) if page_match else 0

            metadata = {
                'chunk_id': len(chunks),
                'page_number': page_num,
                'word_count': len(chunk_words),
                'start_index': i
            }

            chunks.append((chunk_text, metadata))

        return chunks

    def create_faiss_index(self, texts: List[str]) -> faiss.IndexFlatIP:
        """Create FAISS index from text embeddings"""
        print("ðŸ” Creating embeddings...")
        embeddings = self.embedding_model.encode(texts, convert_to_tensor=False)
        embeddings = np.array(embeddings).astype('float32')

        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)

        # Create FAISS index
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
        index.add(embeddings)

        self.embeddings = embeddings
        return index

    def search_similar_chunks(self, query: str, top_k: int = 3) -> List[Tuple[str, float, Dict]]:
        """Search for similar chunks using FAISS"""
        if self.faiss_index is None:
            return []

        # Encode query
        query_embedding = self.embedding_model.encode([query], convert_to_tensor=False)
        query_embedding = np.array(query_embedding).astype('float32')
        faiss.normalize_L2(query_embedding)

        # Search
        scores, indices = self.faiss_index.search(query_embedding, top_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.document_chunks):
                chunk_text = self.document_chunks[idx]
                metadata = self.chunk_metadata[idx]
                results.append((chunk_text, float(score), metadata))

        return results

    def query_granite_model(self, prompt: str, max_tokens: int = 512) -> str:
        """Query IBM Granite model via Hugging Face API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_tokens,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "return_full_text": False
            }
        }

        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()

            result = response.json()

            if isinstance(result, list) and len(result) > 0:
                return result[0].get('generated_text', 'No response generated.')
            elif isinstance(result, dict):
                return result.get('generated_text', 'No response generated.')
            else:
                return 'Unexpected response format from API.'

        except requests.exceptions.RequestException as e:
            return f"API Error: {str(e)}"
        except Exception as e:
            return f"Error processing response: {str(e)}"

    def generate_answer(self, question: str, context_chunks: List[str]) -> str:
        """Generate answer using retrieved context and Granite model"""
        # Combine context chunks
        context = "\n\n".join(context_chunks[:3])  # Limit context to avoid token limits

        # Create prompt for Granite model
        prompt = f"""Based on the following context from academic materials, provide a comprehensive and accurate answer to the question. If the context doesn't contain enough information to answer the question, please say so.

Context:
{context}

Question: {question}

Answer:"""

        return self.query_granite_model(prompt)

    def process_pdf(self, pdf_file) -> str:
        """Process uploaded PDF file"""
        try:
            if pdf_file is None:
                return "âŒ Please upload a PDF file first."

            print(f"ðŸ“„ Processing PDF: {pdf_file.name}")

            # Extract text
            text = self.extract_text_from_pdf(pdf_file.name)
            if text.startswith("Error"):
                return f"âŒ {text}"

            # Preprocess text
            cleaned_text = self.preprocess_text(text)

            # Create chunks
            chunks_with_metadata = self.create_chunks(cleaned_text)
            self.document_chunks = [chunk[0] for chunk in chunks_with_metadata]
            self.chunk_metadata = [chunk[1] for chunk in chunks_with_metadata]

            # Create FAISS index
            self.faiss_index = self.create_faiss_index(self.document_chunks)

            return f"âœ… PDF processed successfully!\nðŸ“Š Created {len(self.document_chunks)} chunks for analysis."

        except Exception as e:
            return f"âŒ Error processing PDF: {str(e)}"

    def answer_question(self, question: str) -> str:
        """Answer question based on processed PDF"""
        try:
            if not question.strip():
                return "âŒ Please enter a question."

            if self.faiss_index is None:
                return "âŒ Please upload and process a PDF first."

            print(f"ðŸ” Searching for: {question}")

            # Search for relevant chunks
            similar_chunks = self.search_similar_chunks(question, top_k=5)

            if not similar_chunks:
                return "âŒ No relevant content found in the document."

            # Extract chunk texts and metadata
            context_chunks = [chunk[0] for chunk in similar_chunks]

            # Generate answer
            print("ðŸ¤– Generating answer with Granite model...")
            answer = self.generate_answer(question, context_chunks)

            # Format response with references
            response = f"**Answer:**\n{answer}\n\n"
            response += "**Sources:**\n"

            for i, (chunk, score, metadata) in enumerate(similar_chunks[:3]):
                page_info = f"Page {metadata['page_number']}" if metadata['page_number'] > 0 else "Document"
                response += f"â€¢ {page_info} (Relevance: {score:.3f})\n"

            return response

        except Exception as e:
            return f"âŒ Error answering question: {str(e)}"

    def create_interface(self):
        """Create Gradio interface"""
        with gr.Blocks(
            title="StudyMate - AI-Powered PDF Academic Assistant",
            theme=gr.themes.Soft()
        ) as interface:

            # Header
            gr.Markdown("""
            # ðŸ“š StudyMate - AI-Powered PDF Academic Assistant

            Upload your academic PDFs (textbooks, lecture notes, research papers) and ask questions in natural language.
            StudyMate uses IBM Granite AI model to provide contextual answers based on your study materials.

            **Features:**
            - ðŸ“„ Smart PDF text extraction
            - ðŸ” Semantic search with FAISS
            - ðŸ¤– AI-powered answers using IBM Granite model
            - ðŸ“– Source references for reliability
            """)

            with gr.Row():
                with gr.Column(scale=1):
                    # PDF Upload Section
                    gr.Markdown("### 1ï¸âƒ£ Upload Your PDF")
                    pdf_input = gr.File(
                        label="Choose PDF File",
                        file_types=[".pdf"],
                        type="filepath"
                    )

                    process_btn = gr.Button(
                        "ðŸ“„ Process PDF",
                        variant="primary",
                        size="lg"
                    )

                    process_output = gr.Textbox(
                        label="Processing Status",
                        lines=3,
                        interactive=False
                    )

                with gr.Column(scale=1):
                    # Q&A Section
                    gr.Markdown("### 2ï¸âƒ£ Ask Questions")
                    question_input = gr.Textbox(
                        label="Your Question",
                        placeholder="e.g., What is the main concept of machine learning?",
                        lines=2
                    )

                    ask_btn = gr.Button(
                        "ðŸ¤– Get Answer",
                        variant="secondary",
                        size="lg"
                    )

                    answer_output = gr.Textbox(
                        label="StudyMate's Answer",
                        lines=10,
                        interactive=False
                    )

            # Example questions
            gr.Markdown("""
            ### ðŸ’¡ Example Questions You Can Ask:
            - "What are the key principles mentioned in Chapter 3?"
            - "Explain the methodology described in this paper"
            - "What are the main findings or conclusions?"
            - "Define the term mentioned on page 15"
            - "Summarize the introduction section"

            ### ðŸ”§ **Troubleshooting Tips:**
            - **If you get timeouts:** Try shorter, more specific questions
            - **For better results:** Ask about specific pages or sections
            - **If API fails:** The system will still show you relevant content from your PDF
            - **Model loading:** Wait 2-3 minutes after first use for optimal performance
            """)

            # Event handlers
            process_btn.click(
                fn=self.process_pdf,
                inputs=[pdf_input],
                outputs=[process_output]
            )

            ask_btn.click(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

            # Allow Enter key to submit question
            question_input.submit(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

        return interface

# Initialize and launch StudyMate
def launch_studymate():
    """Launch StudyMate application"""
    print("ðŸŽ“ Starting StudyMate - AI-Powered PDF Academic Assistant")
    print("=" * 60)

    # Create StudyMate instance
    studymate = StudyMateApp()

    # Create and launch interface
    interface = studymate.create_interface()

    # Launch with public sharing enabled for Colab
    interface.launch(
        share=True,  # Creates public link for Colab
        debug=True,
        height=800,
        show_error=True
    )

# Run the application
if __name__ == "__main__":
    launch_studymate()

# StudyMate - AI-Powered PDF-Based Academic Assistant
# Designed for Google Colab with IBM Granite Model

# Install required packages
!pip install pymupdf sentence-transformers faiss-cpu gradio transformers torch accelerate

import os
import fitz  # PyMuPDF
import faiss
import numpy as np
import gradio as gr
import requests
import json
from sentence_transformers import SentenceTransformer
from typing import List, Tuple, Dict
import re
from datetime import datetime

class StudyMateApp:
    def __init__(self):
        """Initialize StudyMate with necessary components"""
        print("ðŸš€ Initializing StudyMate...")

        # IBM Granite API configuration
        self.api_key = "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"
        self.model_name = "ibm-granite/granite-3.3-2b-instruct"
        self.api_url = f"https://api-inference.huggingface.co/models/{self.model_name}"

        # Initialize embedding model
        print("ðŸ“š Loading sentence transformer...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

        # Storage for documents and embeddings
        self.document_chunks = []
        self.chunk_metadata = []
        self.faiss_index = None
        self.embeddings = None

        print("âœ… StudyMate initialized successfully!")

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF using PyMuPDF"""
        try:
            doc = fitz.open(pdf_path)
            text = ""

            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                text += f"\n--- Page {page_num + 1} ---\n{page_text}"

            doc.close()
            return text
        except Exception as e:
            return f"Error extracting text from PDF: {str(e)}"

    def preprocess_text(self, text: str) -> str:
        """Clean and preprocess extracted text"""
        # Remove excessive whitespace
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r' +', ' ', text)

        # Remove special characters that might interfere
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)]', ' ', text)

        return text.strip()

    def create_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Tuple[str, Dict]]:
        """Split text into overlapping chunks with metadata"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk_text = ' '.join(chunk_words)

            # Extract page number if available
            page_match = re.search(r'--- Page (\d+) ---', chunk_text)
            page_num = int(page_match.group(1)) if page_match else 0

            metadata = {
                'chunk_id': len(chunks),
                'page_number': page_num,
                'word_count': len(chunk_words),
                'start_index': i
            }

            chunks.append((chunk_text, metadata))

        return chunks

    def create_faiss_index(self, texts: List[str]) -> faiss.IndexFlatIP:
        """Create FAISS index from text embeddings"""
        print("ðŸ” Creating embeddings...")
        embeddings = self.embedding_model.encode(texts, convert_to_tensor=False)
        embeddings = np.array(embeddings).astype('float32')

        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)

        # Create FAISS index
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
        index.add(embeddings)

        self.embeddings = embeddings
        return index

    def search_similar_chunks(self, query: str, top_k: int = 3) -> List[Tuple[str, float, Dict]]:
        """Search for similar chunks using FAISS"""
        if self.faiss_index is None:
            return []

        # Encode query
        query_embedding = self.embedding_model.encode([query], convert_to_tensor=False)
        query_embedding = np.array(query_embedding).astype('float32')
        faiss.normalize_L2(query_embedding)

        # Search
        scores, indices = self.faiss_index.search(query_embedding, top_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.document_chunks):
                chunk_text = self.document_chunks[idx]
                metadata = self.chunk_metadata[idx]
                results.append((chunk_text, float(score), metadata))

        return results

    def query_granite_model(self, prompt: str, max_tokens: int = 512) -> str:
        """Query IBM Granite model via Hugging Face API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_tokens,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "return_full_text": False
            }
        }

        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()

            result = response.json()

            if isinstance(result, list) and len(result) > 0:
                return result[0].get('generated_text', 'No response generated.')
            elif isinstance(result, dict):
                return result.get('generated_text', 'No response generated.')
            else:
                return 'Unexpected response format from API.'

        except requests.exceptions.RequestException as e:
            return f"API Error: {str(e)}"
        except Exception as e:
            return f"Error processing response: {str(e)}"

    def generate_answer(self, question: str, context_chunks: List[str]) -> str:
        """Generate answer using retrieved context and Granite model"""
        # Combine context chunks
        context = "\n\n".join(context_chunks[:3])  # Limit context to avoid token limits

        # Create prompt for Granite model
        prompt = f"""Based on the following context from academic materials, provide a comprehensive and accurate answer to the question. If the context doesn't contain enough information to answer the question, please say so.

Context:
{context}

Question: {question}

Answer:"""

        return self.query_granite_model(prompt)

    def process_pdf(self, pdf_file) -> str:
        """Process uploaded PDF file"""
        try:
            if pdf_file is None:
                return "âŒ Please upload a PDF file first."

            print(f"ðŸ“„ Processing PDF: {pdf_file.name}")

            # Extract text
            text = self.extract_text_from_pdf(pdf_file.name)
            if text.startswith("Error"):
                return f"âŒ {text}"

            # Preprocess text
            cleaned_text = self.preprocess_text(text)

            # Create chunks
            chunks_with_metadata = self.create_chunks(cleaned_text)
            self.document_chunks = [chunk[0] for chunk in chunks_with_metadata]
            self.chunk_metadata = [chunk[1] for chunk in chunks_with_metadata]

            # Create FAISS index
            self.faiss_index = self.create_faiss_index(self.document_chunks)

            return f"âœ… PDF processed successfully!\nðŸ“Š Created {len(self.document_chunks)} chunks for analysis."

        except Exception as e:
            return f"âŒ Error processing PDF: {str(e)}"

    def answer_question(self, question: str) -> str:
        """Answer question based on processed PDF"""
        try:
            if not question.strip():
                return "âŒ Please enter a question."

            if self.faiss_index is None:
                return "âŒ Please upload and process a PDF first."

            print(f"ðŸ” Searching for: {question}")

            # Search for relevant chunks
            similar_chunks = self.search_similar_chunks(question, top_k=5)

            if not similar_chunks:
                return "âŒ No relevant content found in the document."

            # Extract chunk texts and metadata
            context_chunks = [chunk[0] for chunk in similar_chunks]

            # Generate answer
            print("ðŸ¤– Generating answer with Granite model...")
            answer = self.generate_answer(question, context_chunks)

            # Format response with references
            response = f"**Answer:**\n{answer}\n\n"
            response += "**Sources:**\n"

            for i, (chunk, score, metadata) in enumerate(similar_chunks[:3]):
                page_info = f"Page {metadata['page_number']}" if metadata['page_number'] > 0 else "Document"
                response += f"â€¢ {page_info} (Relevance: {score:.3f})\n"

            return response

        except Exception as e:
            return f"âŒ Error answering question: {str(e)}"

    def create_interface(self):
        """Create Gradio interface"""
        with gr.Blocks(
            title="StudyMate - AI-Powered PDF Academic Assistant",
            theme=gr.themes.Soft()
        ) as interface:

            # Header
            gr.Markdown("""
            # ðŸ“š StudyMate - AI-Powered PDF Academic Assistant

            Upload your academic PDFs (textbooks, lecture notes, research papers) and ask questions in natural language.
            StudyMate uses IBM Granite AI model to provide contextual answers based on your study materials.

            **Features:**
            - ðŸ“„ Smart PDF text extraction
            - ðŸ” Semantic search with FAISS
            - ðŸ¤– AI-powered answers using IBM Granite model
            - ðŸ“– Source references for reliability
            """)

            with gr.Row():
                with gr.Column(scale=1):
                    # PDF Upload Section
                    gr.Markdown("### 1ï¸âƒ£ Upload Your PDF")
                    pdf_input = gr.File(
                        label="Choose PDF File",
                        file_types=[".pdf"],
                        type="filepath"
                    )

                    process_btn = gr.Button(
                        "ðŸ“„ Process PDF",
                        variant="primary",
                        size="lg"
                    )

                    process_output = gr.Textbox(
                        label="Processing Status",
                        lines=3,
                        interactive=False
                    )

                with gr.Column(scale=1):
                    # Q&A Section
                    gr.Markdown("### 2ï¸âƒ£ Ask Questions")
                    question_input = gr.Textbox(
                        label="Your Question",
                        placeholder="e.g., What is the main concept of machine learning?",
                        lines=2
                    )

                    ask_btn = gr.Button(
                        "ðŸ¤– Get Answer",
                        variant="secondary",
                        size="lg"
                    )

                    answer_output = gr.Textbox(
                        label="StudyMate's Answer",
                        lines=10,
                        interactive=False
                    )

            # Example questions
            gr.Markdown("""
            ### ðŸ’¡ Example Questions You Can Ask:
            - "What are the key principles mentioned in Chapter 3?"
            - "Explain the methodology described in this paper"
            - "What are the main findings or conclusions?"
            - "Define the term mentioned on page 15"
            - "Summarize the introduction section"

            ### ðŸ”§ **Troubleshooting Tips:**
            - **If you get timeouts:** Try shorter, more specific questions
            - **For better results:** Ask about specific pages or sections
            - **If API fails:** The system will still show you relevant content from your PDF
            - **Model loading:** Wait 2-3 minutes after first use for optimal performance
            """)

            # Event handlers
            process_btn.click(
                fn=self.process_pdf,
                inputs=[pdf_input],
                outputs=[process_output]
            )

            ask_btn.click(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

            # Allow Enter key to submit question
            question_input.submit(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

        return interface

# Initialize and launch StudyMate
def launch_studymate():
    """Launch StudyMate application"""
    print("ðŸŽ“ Starting StudyMate - AI-Powered PDF Academic Assistant")
    print("=" * 60)

    # Create StudyMate instance
    studymate = StudyMateApp()

    # Create and launch interface
    interface = studymate.create_interface()

    # Launch with public sharing enabled for Colab
    interface.launch(
        share=True,  # Creates public link for Colab
        debug=True,
        height=800,
        show_error=True
    )

# Run the application
if __name__ == "__main__":
    launch_studymate()



# StudyMate - AI-Powered PDF-Based Academic Assistant
# Designed for Google Colab with Multiple AI Model Options

# Install required packages
!pip install pymupdf sentence-transformers faiss-cpu gradio transformers torch accelerate

import os
import fitz  # PyMuPDF
import faiss
import numpy as np
import gradio as gr
import requests
import json
from sentence_transformers import SentenceTransformer
from typing import List, Tuple, Dict
import re
from datetime import datetime

class StudyMateApp:
    def __init__(self):
        """Initialize StudyMate with multiple AI model options"""
        print("ðŸš€ Initializing StudyMate...")

        # Primary API configuration - Hugging Face
        self.api_key = "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"

        # Multiple model options with fallbacks
        self.model_configs = [
            {
                "name": "microsoft/DialoGPT-medium",
                "url": "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium",
                "type": "huggingface"
            },
            {
                "name": "facebook/blenderbot-400M-distill",
                "url": "https://api-inference.huggingface.co/models/facebook/blenderbot-400M-distill",
                "type": "huggingface"
            },
            {
                "name": "google/flan-t5-base",
                "url": "https://api-inference.huggingface.co/models/google/flan-t5-base",
                "type": "huggingface"
            }
        ]

        self.current_model_index = 0
        self.working_model = None

        # Test and find working model
        print("ðŸ” Testing API models...")
        self._find_working_model()

        # Initialize embedding model
        print("ðŸ“š Loading sentence transformer...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

        # Storage for documents and embeddings
        self.document_chunks = []
        self.chunk_metadata = []
        self.faiss_index = None
        self.embeddings = None

        print("âœ… StudyMate initialized successfully!")

    def _find_working_model(self):
        """Test different models to find one that works"""
        for i, config in enumerate(self.model_configs):
            print(f"Testing {config['name']}...")
            if self._test_model(config):
                self.working_model = config
                self.current_model_index = i
                print(f"âœ… Found working model: {config['name']}")
                return

        print("âš ï¸ No working API model found. Will use local fallback responses.")
        self.working_model = None

    def _test_model(self, model_config: dict) -> bool:
        """Test if a model is working"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        test_payload = {
            "inputs": "Hello, how are you?",
            "parameters": {
                "max_new_tokens": 10,
                "temperature": 0.1
            },
            "options": {
                "wait_for_model": True
            }
        }

        try:
            response = requests.post(
                model_config["url"],
                headers=headers,
                json=test_payload,
                timeout=15
            )

            if response.status_code == 200:
                result = response.json()
                if isinstance(result, (list, dict)) and result:
                    return True
            elif response.status_code == 503:
                # Model is loading, still valid
                return True
        except:
            pass

        return False

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF using PyMuPDF"""
        try:
            doc = fitz.open(pdf_path)
            text = ""

            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                text += f"\n--- Page {page_num + 1} ---\n{page_text}"

            doc.close()
            return text
        except Exception as e:
            return f"Error extracting text from PDF: {str(e)}"

    def preprocess_text(self, text: str) -> str:
        """Clean and preprocess extracted text"""
        # Remove excessive whitespace
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r' +', ' ', text)

        # Remove special characters that might interfere
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)]', ' ', text)

        return text.strip()

    def create_chunks(self, text: str, chunk_size: int = 800, overlap: int = 150) -> List[Tuple[str, Dict]]:
        """Split text into overlapping chunks with metadata"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk_text = ' '.join(chunk_words)

            # Extract page number if available
            page_match = re.search(r'--- Page (\d+) ---', chunk_text)
            page_num = int(page_match.group(1)) if page_match else 0

            metadata = {
                'chunk_id': len(chunks),
                'page_number': page_num,
                'word_count': len(chunk_words),
                'start_index': i
            }

            chunks.append((chunk_text, metadata))

        return chunks

    def create_faiss_index(self, texts: List[str]) -> faiss.IndexFlatIP:
        """Create FAISS index from text embeddings"""
        print("ðŸ” Creating embeddings...")
        embeddings = self.embedding_model.encode(texts, convert_to_tensor=False)
        embeddings = np.array(embeddings).astype('float32')

        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)

        # Create FAISS index
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
        index.add(embeddings)

        self.embeddings = embeddings
        return index

    def search_similar_chunks(self, query: str, top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        """Search for similar chunks using FAISS"""
        if self.faiss_index is None:
            return []

        # Encode query
        query_embedding = self.embedding_model.encode([query], convert_to_tensor=False)
        query_embedding = np.array(query_embedding).astype('float32')
        faiss.normalize_L2(query_embedding)

        # Search
        scores, indices = self.faiss_index.search(query_embedding, top_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.document_chunks):
                chunk_text = self.document_chunks[idx]
                metadata = self.chunk_metadata[idx]
                results.append((chunk_text, float(score), metadata))

        return results

    def query_ai_model(self, prompt: str, max_tokens: int = 150) -> str:
        """Query AI model with multiple fallback options"""
        if not self.working_model:
            return self._generate_rule_based_answer(prompt)

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        # Optimize prompt
        if len(prompt) > 800:
            prompt = prompt[:800] + "..."

        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_tokens,
                "temperature": 0.4,
                "top_p": 0.9,
                "do_sample": True,
                "return_full_text": False
            },
            "options": {
                "wait_for_model": True,
                "use_cache": True
            }
        }

        try:
            response = requests.post(
                self.working_model["url"],
                headers=headers,
                json=payload,
                timeout=25
            )

            if response.status_code == 200:
                result = response.json()

                if isinstance(result, list) and len(result) > 0:
                    generated_text = result[0].get('generated_text', '').strip()
                    if generated_text and len(generated_text) > 10:
                        return generated_text
                elif isinstance(result, dict):
                    generated_text = result.get('generated_text', '').strip()
                    if generated_text and len(generated_text) > 10:
                        return generated_text

            elif response.status_code == 503:
                return "â³ Model is loading, please try again in a moment..."
            elif response.status_code == 401:
                return "ðŸ”‘ API authentication issue. Using local processing..."
            else:
                return f"âš ï¸ API returned status {response.status_code}"

        except Exception as e:
            print(f"API Error: {str(e)}")

        # Fallback to rule-based response
        return self._generate_rule_based_answer(prompt)

    def _generate_rule_based_answer(self, prompt: str) -> str:
        """Generate answers using rule-based approach when API fails"""
        # Extract question from prompt
        question_match = re.search(r'Question:\s*(.+?)(?:\n|$)', prompt, re.IGNORECASE)
        question = question_match.group(1) if question_match else "your question"

        # Extract context
        context_match = re.search(r'Context.*?:(.*?)Question:', prompt, re.DOTALL | re.IGNORECASE)
        context = context_match.group(1).strip() if context_match else ""

        if not context:
            return f"I found relevant content in your PDF, but I cannot generate an AI response right now. Please review the source sections below."

        # Simple keyword extraction and response
        context_lower = context.lower()
        question_lower = question.lower()

        # Get key sentences that might answer the question
        sentences = [s.strip() for s in context.split('.') if s.strip()]
        relevant_sentences = []

        # Find sentences with overlapping words
        question_words = set(re.findall(r'\b\w+\b', question_lower))

        for sentence in sentences:
            sentence_words = set(re.findall(r'\b\w+\b', sentence.lower()))
            overlap = len(question_words.intersection(sentence_words))
            if overlap > 0 and len(sentence) > 20:
                relevant_sentences.append((sentence, overlap))

        # Sort by relevance
        relevant_sentences.sort(key=lambda x: x[1], reverse=True)

        if relevant_sentences:
            best_sentences = [s[0] for s in relevant_sentences[:2]]
            answer = ". ".join(best_sentences) + "."
            return f"Based on the document content: {answer}"
        else:
            return "The document contains relevant information, but I cannot provide a specific answer right now. Please review the source sections below."

    def generate_answer(self, question: str, context_chunks: List[str]) -> str:
        """Generate answer using retrieved context"""
        try:
            if len(context_chunks) > 0:
                # Use the most relevant context
                primary_context = context_chunks[0][:600]

                prompt = f"""Context from document: {primary_context}

Question: {question}

Based on the context, provide a clear answer:"""

                print("ðŸ¤– Generating answer...")
                answer = self.query_ai_model(prompt, max_tokens=120)

                return answer
            else:
                return "âŒ No relevant content found in the document for your question."

        except Exception as e:
            print(f"Error generating answer: {str(e)}")
            return self._generate_rule_based_answer(f"Context: {context_chunks[0] if context_chunks else ''}\nQuestion: {question}")

    def process_pdf(self, pdf_file) -> str:
        """Process uploaded PDF file"""
        try:
            if pdf_file is None:
                return "âŒ Please upload a PDF file first."

            print(f"ðŸ“„ Processing PDF: {pdf_file.name}")

            # Extract text
            text = self.extract_text_from_pdf(pdf_file.name)
            if text.startswith("Error"):
                return f"âŒ {text}"

            # Preprocess text
            cleaned_text = self.preprocess_text(text)

            if len(cleaned_text) < 100:
                return "âŒ The PDF appears to be empty or contains very little text. Please upload a different PDF."

            # Create chunks
            chunks_with_metadata = self.create_chunks(cleaned_text)
            self.document_chunks = [chunk[0] for chunk in chunks_with_metadata]
            self.chunk_metadata = [chunk[1] for chunk in chunks_with_metadata]

            # Create FAISS index
            self.faiss_index = self.create_faiss_index(self.document_chunks)

            status_msg = f"âœ… PDF processed successfully!\n"
            status_msg += f"ðŸ“Š Created {len(self.document_chunks)} chunks for analysis.\n"
            status_msg += f"ðŸ” Ready to answer questions about your document!\n"

            if self.working_model:
                status_msg += f"ðŸ¤– Using AI model: {self.working_model['name']}"
            else:
                status_msg += f"ðŸ’¡ Using intelligent text processing (AI model unavailable)"

            return status_msg

        except Exception as e:
            return f"âŒ Error processing PDF: {str(e)}"

    def answer_question(self, question: str) -> str:
        """Answer question based on processed PDF"""
        try:
            if not question.strip():
                return "âŒ Please enter a question."

            if self.faiss_index is None:
                return "âŒ Please upload and process a PDF first."

            print(f"ðŸ” Searching for: {question}")

            # Search for relevant chunks
            similar_chunks = self.search_similar_chunks(question, top_k=5)

            if not similar_chunks:
                return "âŒ No relevant content found in the document for your question. Try rephrasing or asking about different topics."

            print(f"âœ… Found {len(similar_chunks)} relevant sections")

            # Extract chunk texts
            context_chunks = [chunk[0] for chunk in similar_chunks]

            # Generate answer
            answer = self.generate_answer(question, context_chunks)

            # Format response
            response = f"**ðŸ¤– Answer:**\n{answer}\n\n"
            response += "**ðŸ“‘ Source References:**\n"

            for i, (chunk, score, metadata) in enumerate(similar_chunks[:3]):
                page_info = f"Page {metadata['page_number']}" if metadata['page_number'] > 0 else "Document"
                confidence = "High" if score > 0.3 else "Medium" if score > 0.15 else "Low"
                response += f"â€¢ {page_info} - Confidence: {confidence} (Score: {score:.3f})\n"

            # Add a snippet of the most relevant content
            response += f"\n**ðŸ“‹ Most Relevant Content:**\n"
            response += f"*{similar_chunks[0][0][:200]}...*"

            return response

        except Exception as e:
            error_msg = str(e)
            print(f"Error in answer_question: {error_msg}")
            return f"""âŒ **Error processing your question:**

{error_msg}

**ðŸ’¡ What you can try:**
1. Make sure your PDF was uploaded successfully
2. Ask a simpler or more specific question
3. Try asking about specific topics mentioned in your document
4. Check if your question relates to the content of the PDF

**The system is still working** - the search function found relevant content, but there was an issue generating the response."""

    def create_interface(self):
        """Create Gradio interface"""
        with gr.Blocks(
            title="StudyMate - AI-Powered PDF Academic Assistant",
            theme=gr.themes.Soft(),
            css="""
            .gradio-container {
                max-width: 1200px;
                margin: auto;
            }
            .header {
                text-align: center;
                margin-bottom: 20px;
            }
            """
        ) as interface:

            # Header
            gr.Markdown("""
            <div class="header">

            # ðŸ“š StudyMate - AI-Powered PDF Academic Assistant

            **Upload your academic PDFs and ask questions in natural language!**

            âœ¨ **Features:**
            - ðŸ“„ Smart PDF text extraction and processing
            - ðŸ” Semantic search with FAISS vector search
            - ðŸ¤– AI-powered answers with multiple model fallbacks
            - ðŸ“– Source references and confidence scoring
            - ðŸ›¡ï¸ Robust error handling and offline capabilities

            </div>
            """)

            # Status indicator
            model_status = "ðŸ¤– AI Model Ready" if self.working_model else "ðŸ’¡ Smart Text Processing Mode"
            gr.Markdown(f"**Current Status:** {model_status}")

            with gr.Row():
                with gr.Column(scale=1):
                    # PDF Upload Section
                    gr.Markdown("### ðŸ“¤ Step 1: Upload Your PDF")
                    pdf_input = gr.File(
                        label="Choose PDF File (textbooks, papers, notes)",
                        file_types=[".pdf"],
                        type="filepath"
                    )

                    process_btn = gr.Button(
                        "ðŸ”„ Process PDF",
                        variant="primary",
                        size="lg"
                    )

                    process_output = gr.Textbox(
                        label="ðŸ“Š Processing Status",
                        lines=4,
                        interactive=False
                    )

                with gr.Column(scale=1):
                    # Q&A Section
                    gr.Markdown("### â“ Step 2: Ask Questions")
                    question_input = gr.Textbox(
                        label="Your Question",
                        placeholder="e.g., What is the main concept explained in this document?",
                        lines=2
                    )

                    ask_btn = gr.Button(
                        "ðŸ” Get Answer",
                        variant="secondary",
                        size="lg"
                    )

                    answer_output = gr.Textbox(
                        label="ðŸ“ StudyMate's Response",
                        lines=12,
                        interactive=False
                    )

            # Help section
            with gr.Accordion("ðŸ’¡ Help & Tips", open=False):
                gr.Markdown("""
                ### ðŸ“‹ How to Use:
                1. **Upload a PDF**: Choose your academic document (textbook, research paper, lecture notes)
                2. **Wait for Processing**: The system will extract and index the text
                3. **Ask Questions**: Use natural language to ask about the content

                ### ðŸŽ¯ Example Questions:
                - "What is the main argument in this paper?"
                - "Explain the concept mentioned on page 5"
                - "What are the key findings or conclusions?"
                - "Define the term [specific term] from the document"
                - "Summarize the methodology section"
                - "What does the author say about [topic]?"

                ### ðŸ”§ Troubleshooting:
                - **No relevant content found**: Try rephrasing your question or asking about different topics
                - **API issues**: The system will still provide relevant content from your PDF
                - **Long processing time**: Large PDFs may take a few minutes to process
                - **Low confidence scores**: Try asking more specific questions

                ### âš¡ Tips for Better Results:
                - Ask specific questions rather than very broad ones
                - Reference page numbers if you know them
                - Use keywords that appear in your document
                - Break complex questions into simpler parts
                """)

            # Event handlers
            process_btn.click(
                fn=self.process_pdf,
                inputs=[pdf_input],
                outputs=[process_output]
            )

            ask_btn.click(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

            # Allow Enter key to submit question
            question_input.submit(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

        return interface

# Initialize and launch StudyMate
def launch_studymate():
    """Launch StudyMate application"""
    print("ðŸŽ“ Starting StudyMate - AI-Powered PDF Academic Assistant")
    print("=" * 60)

    try:
        # Create StudyMate instance
        studymate = StudyMateApp()

        # Create and launch interface
        interface = studymate.create_interface()

        # Launch with public sharing enabled for Colab
        print("ðŸŒ Launching interface...")
        interface.launch(
            share=True,  # Creates public link for Colab
            debug=False,
            height=900,
            show_error=True,
            server_name="0.0.0.0",
            server_port=7860
        )

    except Exception as e:
        print(f"âŒ Error launching StudyMate: {str(e)}")
        print("ðŸ’¡ Try restarting the notebook and running the code again.")

# Run the application
if __name__ == "__main__":
    launch_studymate()