# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

# StudyMate - AI-Powered PDF-Based Academic Assistant
# Designed for Google Colab with IBM Granite Model

# Install required packages
!pip install pymupdf sentence-transformers faiss-cpu gradio transformers torch accelerate

import os
import fitz  # PyMuPDF
import faiss
import numpy as np
import gradio as gr
import requests
import json
from sentence_transformers import SentenceTransformer
from typing import List, Tuple, Dict
import re
from datetime import datetime

class StudyMateApp:
    def __init__(self):
        """Initialize StudyMate with necessary components"""
        print("üöÄ Initializing StudyMate...")

        # IBM Granite API configuration
        self.api_key = "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"
        self.model_name = "ibm-granite/granite-3.3-2b-instruct"
        self.api_url = f"https://api-inference.huggingface.co/models/{self.model_name}"

        # Initialize embedding model
        print("üìö Loading sentence transformer...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

        # Storage for documents and embeddings
        self.document_chunks = []
        self.chunk_metadata = []
        self.faiss_index = None
        self.embeddings = None

        print("‚úÖ StudyMate initialized successfully!")

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF using PyMuPDF"""
        try:
            doc = fitz.open(pdf_path)
            text = ""

            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                text += f"\n--- Page {page_num + 1} ---\n{page_text}"

            doc.close()
            return text
        except Exception as e:
            return f"Error extracting text from PDF: {str(e)}"

    def preprocess_text(self, text: str) -> str:
        """Clean and preprocess extracted text"""
        # Remove excessive whitespace
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r' +', ' ', text)

        # Remove special characters that might interfere
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)]', ' ', text)

        return text.strip()

    def create_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Tuple[str, Dict]]:
        """Split text into overlapping chunks with metadata"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk_text = ' '.join(chunk_words)

            # Extract page number if available
            page_match = re.search(r'--- Page (\d+) ---', chunk_text)
            page_num = int(page_match.group(1)) if page_match else 0

            metadata = {
                'chunk_id': len(chunks),
                'page_number': page_num,
                'word_count': len(chunk_words),
                'start_index': i
            }

            chunks.append((chunk_text, metadata))

        return chunks

    def create_faiss_index(self, texts: List[str]) -> faiss.IndexFlatIP:
        """Create FAISS index from text embeddings"""
        print("üîç Creating embeddings...")
        embeddings = self.embedding_model.encode(texts, convert_to_tensor=False)
        embeddings = np.array(embeddings).astype('float32')

        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)

        # Create FAISS index
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
        index.add(embeddings)

        self.embeddings = embeddings
        return index

    def search_similar_chunks(self, query: str, top_k: int = 3) -> List[Tuple[str, float, Dict]]:
        """Search for similar chunks using FAISS"""
        if self.faiss_index is None:
            return []

        # Encode query
        query_embedding = self.embedding_model.encode([query], convert_to_tensor=False)
        query_embedding = np.array(query_embedding).astype('float32')
        faiss.normalize_L2(query_embedding)

        # Search
        scores, indices = self.faiss_index.search(query_embedding, top_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.document_chunks):
                chunk_text = self.document_chunks[idx]
                metadata = self.chunk_metadata[idx]
                results.append((chunk_text, float(score), metadata))

        return results

    def query_granite_model(self, prompt: str, max_tokens: int = 512) -> str:
        """Query IBM Granite model via Hugging Face API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_tokens,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "return_full_text": False
            }
        }

        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()

            result = response.json()

            if isinstance(result, list) and len(result) > 0:
                return result[0].get('generated_text', 'No response generated.')
            elif isinstance(result, dict):
                return result.get('generated_text', 'No response generated.')
            else:
                return 'Unexpected response format from API.'

        except requests.exceptions.RequestException as e:
            return f"API Error: {str(e)}"
        except Exception as e:
            return f"Error processing response: {str(e)}"

    def generate_answer(self, question: str, context_chunks: List[str]) -> str:
        """Generate answer using retrieved context and Granite model"""
        # Combine context chunks
        context = "\n\n".join(context_chunks[:3])  # Limit context to avoid token limits

        # Create prompt for Granite model
        prompt = f"""Based on the following context from academic materials, provide a comprehensive and accurate answer to the question. If the context doesn't contain enough information to answer the question, please say so.

Context:
{context}

Question: {question}

Answer:"""

        return self.query_granite_model(prompt)

    def process_pdf(self, pdf_file) -> str:
        """Process uploaded PDF file"""
        try:
            if pdf_file is None:
                return "‚ùå Please upload a PDF file first."

            print(f"üìÑ Processing PDF: {pdf_file.name}")

            # Extract text
            text = self.extract_text_from_pdf(pdf_file.name)
            if text.startswith("Error"):
                return f"‚ùå {text}"

            # Preprocess text
            cleaned_text = self.preprocess_text(text)

            # Create chunks
            chunks_with_metadata = self.create_chunks(cleaned_text)
            self.document_chunks = [chunk[0] for chunk in chunks_with_metadata]
            self.chunk_metadata = [chunk[1] for chunk in chunks_with_metadata]

            # Create FAISS index
            self.faiss_index = self.create_faiss_index(self.document_chunks)

            return f"‚úÖ PDF processed successfully!\nüìä Created {len(self.document_chunks)} chunks for analysis."

        except Exception as e:
            return f"‚ùå Error processing PDF: {str(e)}"

    def answer_question(self, question: str) -> str:
        """Answer question based on processed PDF"""
        try:
            if not question.strip():
                return "‚ùå Please enter a question."

            if self.faiss_index is None:
                return "‚ùå Please upload and process a PDF first."

            print(f"üîç Searching for: {question}")

            # Search for relevant chunks
            similar_chunks = self.search_similar_chunks(question, top_k=5)

            if not similar_chunks:
                return "‚ùå No relevant content found in the document."

            # Extract chunk texts and metadata
            context_chunks = [chunk[0] for chunk in similar_chunks]

            # Generate answer
            print("ü§ñ Generating answer with Granite model...")
            answer = self.generate_answer(question, context_chunks)

            # Format response with references
            response = f"**Answer:**\n{answer}\n\n"
            response += "**Sources:**\n"

            for i, (chunk, score, metadata) in enumerate(similar_chunks[:3]):
                page_info = f"Page {metadata['page_number']}" if metadata['page_number'] > 0 else "Document"
                response += f"‚Ä¢ {page_info} (Relevance: {score:.3f})\n"

            return response

        except Exception as e:
            return f"‚ùå Error answering question: {str(e)}"

    def create_interface(self):
        """Create Gradio interface"""
        with gr.Blocks(
            title="StudyMate - AI-Powered PDF Academic Assistant",
            theme=gr.themes.Soft()
        ) as interface:

            # Header
            gr.Markdown("""
            # üìö StudyMate - AI-Powered PDF Academic Assistant

            Upload your academic PDFs (textbooks, lecture notes, research papers) and ask questions in natural language.
            StudyMate uses IBM Granite AI model to provide contextual answers based on your study materials.

            **Features:**
            - üìÑ Smart PDF text extraction
            - üîç Semantic search with FAISS
            - ü§ñ AI-powered answers using IBM Granite model
            - üìñ Source references for reliability
            """)

            with gr.Row():
                with gr.Column(scale=1):
                    # PDF Upload Section
                    gr.Markdown("### 1Ô∏è‚É£ Upload Your PDF")
                    pdf_input = gr.File(
                        label="Choose PDF File",
                        file_types=[".pdf"],
                        type="filepath"
                    )

                    process_btn = gr.Button(
                        "üìÑ Process PDF",
                        variant="primary",
                        size="lg"
                    )

                    process_output = gr.Textbox(
                        label="Processing Status",
                        lines=3,
                        interactive=False
                    )

                with gr.Column(scale=1):
                    # Q&A Section
                    gr.Markdown("### 2Ô∏è‚É£ Ask Questions")
                    question_input = gr.Textbox(
                        label="Your Question",
                        placeholder="e.g., What is the main concept of machine learning?",
                        lines=2
                    )

                    ask_btn = gr.Button(
                        "ü§ñ Get Answer",
                        variant="secondary",
                        size="lg"
                    )

                    answer_output = gr.Textbox(
                        label="StudyMate's Answer",
                        lines=10,
                        interactive=False
                    )

            # Example questions
            gr.Markdown("""
            ### üí° Example Questions You Can Ask:
            - "What are the key principles mentioned in Chapter 3?"
            - "Explain the methodology described in this paper"
            - "What are the main findings or conclusions?"
            - "Define the term mentioned on page 15"
            - "Summarize the introduction section"
            """)

            # Event handlers
            process_btn.click(
                fn=self.process_pdf,
                inputs=[pdf_input],
                outputs=[process_output]
            )

            ask_btn.click(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

            # Allow Enter key to submit question
            question_input.submit(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

        return interface

# Initialize and launch StudyMate
def launch_studymate():
    """Launch StudyMate application"""
    print("üéì Starting StudyMate - AI-Powered PDF Academic Assistant")
    print("=" * 60)

    # Create StudyMate instance
    studymate = StudyMateApp()

    # Create and launch interface
    interface = studymate.create_interface()

    # Launch with public sharing enabled for Colab
    interface.launch(
        share=True,  # Creates public link for Colab
        debug=True,
        height=800,
        show_error=True
    )

# Run the application
if __name__ == "__main__":
    launch_studymate()

# StudyMate - AI-Powered PDF-Based Academic Assistant
# Designed for Google Colab with IBM Granite Model

# Install required packages
!pip install pymupdf sentence-transformers faiss-cpu gradio transformers torch accelerate

import os
import fitz  # PyMuPDF
import faiss
import numpy as np
import gradio as gr
import requests
import json
from sentence_transformers import SentenceTransformer
from typing import List, Tuple, Dict
import re
from datetime import datetime

class StudyMateApp:
    def __init__(self):
        """Initialize StudyMate with necessary components"""
        print("üöÄ Initializing StudyMate...")

        # IBM Granite API configuration
        self.api_key = "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"
        self.model_name = "ibm-granite/granite-3.3-2b-instruct"
        self.api_url = f"https://api-inference.huggingface.co/models/{self.model_name}"

        # Initialize embedding model
        print("üìö Loading sentence transformer...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

        # Storage for documents and embeddings
        self.document_chunks = []
        self.chunk_metadata = []
        self.faiss_index = None
        self.embeddings = None

        print("‚úÖ StudyMate initialized successfully!")

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF using PyMuPDF"""
        try:
            doc = fitz.open(pdf_path)
            text = ""

            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                text += f"\n--- Page {page_num + 1} ---\n{page_text}"

            doc.close()
            return text
        except Exception as e:
            return f"Error extracting text from PDF: {str(e)}"

    def preprocess_text(self, text: str) -> str:
        """Clean and preprocess extracted text"""
        # Remove excessive whitespace
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r' +', ' ', text)

        # Remove special characters that might interfere
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)]', ' ', text)

        return text.strip()

    def create_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Tuple[str, Dict]]:
        """Split text into overlapping chunks with metadata"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk_text = ' '.join(chunk_words)

            # Extract page number if available
            page_match = re.search(r'--- Page (\d+) ---', chunk_text)
            page_num = int(page_match.group(1)) if page_match else 0

            metadata = {
                'chunk_id': len(chunks),
                'page_number': page_num,
                'word_count': len(chunk_words),
                'start_index': i
            }

            chunks.append((chunk_text, metadata))

        return chunks

    def create_faiss_index(self, texts: List[str]) -> faiss.IndexFlatIP:
        """Create FAISS index from text embeddings"""
        print("üîç Creating embeddings...")
        embeddings = self.embedding_model.encode(texts, convert_to_tensor=False)
        embeddings = np.array(embeddings).astype('float32')

        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)

        # Create FAISS index
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
        index.add(embeddings)

        self.embeddings = embeddings
        return index

    def search_similar_chunks(self, query: str, top_k: int = 3) -> List[Tuple[str, float, Dict]]:
        """Search for similar chunks using FAISS"""
        if self.faiss_index is None:
            return []

        # Encode query
        query_embedding = self.embedding_model.encode([query], convert_to_tensor=False)
        query_embedding = np.array(query_embedding).astype('float32')
        faiss.normalize_L2(query_embedding)

        # Search
        scores, indices = self.faiss_index.search(query_embedding, top_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.document_chunks):
                chunk_text = self.document_chunks[idx]
                metadata = self.chunk_metadata[idx]
                results.append((chunk_text, float(score), metadata))

        return results

    def query_granite_model(self, prompt: str, max_tokens: int = 512) -> str:
        """Query IBM Granite model via Hugging Face API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_tokens,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "return_full_text": False
            }
        }

        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()

            result = response.json()

            if isinstance(result, list) and len(result) > 0:
                return result[0].get('generated_text', 'No response generated.')
            elif isinstance(result, dict):
                return result.get('generated_text', 'No response generated.')
            else:
                return 'Unexpected response format from API.'

        except requests.exceptions.RequestException as e:
            return f"API Error: {str(e)}"
        except Exception as e:
            return f"Error processing response: {str(e)}"

    def generate_answer(self, question: str, context_chunks: List[str]) -> str:
        """Generate answer using retrieved context and Granite model"""
        # Combine context chunks
        context = "\n\n".join(context_chunks[:3])  # Limit context to avoid token limits

        # Create prompt for Granite model
        prompt = f"""Based on the following context from academic materials, provide a comprehensive and accurate answer to the question. If the context doesn't contain enough information to answer the question, please say so.

Context:
{context}

Question: {question}

Answer:"""

        return self.query_granite_model(prompt)

    def process_pdf(self, pdf_file) -> str:
        """Process uploaded PDF file"""
        try:
            if pdf_file is None:
                return "‚ùå Please upload a PDF file first."

            print(f"üìÑ Processing PDF: {pdf_file.name}")

            # Extract text
            text = self.extract_text_from_pdf(pdf_file.name)
            if text.startswith("Error"):
                return f"‚ùå {text}"

            # Preprocess text
            cleaned_text = self.preprocess_text(text)

            # Create chunks
            chunks_with_metadata = self.create_chunks(cleaned_text)
            self.document_chunks = [chunk[0] for chunk in chunks_with_metadata]
            self.chunk_metadata = [chunk[1] for chunk in chunks_with_metadata]

            # Create FAISS index
            self.faiss_index = self.create_faiss_index(self.document_chunks)

            return f"‚úÖ PDF processed successfully!\nüìä Created {len(self.document_chunks)} chunks for analysis."

        except Exception as e:
            return f"‚ùå Error processing PDF: {str(e)}"

    def answer_question(self, question: str) -> str:
        """Answer question based on processed PDF"""
        try:
            if not question.strip():
                return "‚ùå Please enter a question."

            if self.faiss_index is None:
                return "‚ùå Please upload and process a PDF first."

            print(f"üîç Searching for: {question}")

            # Search for relevant chunks
            similar_chunks = self.search_similar_chunks(question, top_k=5)

            if not similar_chunks:
                return "‚ùå No relevant content found in the document."

            # Extract chunk texts and metadata
            context_chunks = [chunk[0] for chunk in similar_chunks]

            # Generate answer
            print("ü§ñ Generating answer with Granite model...")
            answer = self.generate_answer(question, context_chunks)

            # Format response with references
            response = f"**Answer:**\n{answer}\n\n"
            response += "**Sources:**\n"

            for i, (chunk, score, metadata) in enumerate(similar_chunks[:3]):
                page_info = f"Page {metadata['page_number']}" if metadata['page_number'] > 0 else "Document"
                response += f"‚Ä¢ {page_info} (Relevance: {score:.3f})\n"

            return response

        except Exception as e:
            return f"‚ùå Error answering question: {str(e)}"

    def create_interface(self):
        """Create Gradio interface"""
        with gr.Blocks(
            title="StudyMate - AI-Powered PDF Academic Assistant",
            theme=gr.themes.Soft()
        ) as interface:

            # Header
            gr.Markdown("""
            # üìö StudyMate - AI-Powered PDF Academic Assistant

            Upload your academic PDFs (textbooks, lecture notes, research papers) and ask questions in natural language.
            StudyMate uses IBM Granite AI model to provide contextual answers based on your study materials.

            **Features:**
            - üìÑ Smart PDF text extraction
            - üîç Semantic search with FAISS
            - ü§ñ AI-powered answers using IBM Granite model
            - üìñ Source references for reliability
            """)

            with gr.Row():
                with gr.Column(scale=1):
                    # PDF Upload Section
                    gr.Markdown("### 1Ô∏è‚É£ Upload Your PDF")
                    pdf_input = gr.File(
                        label="Choose PDF File",
                        file_types=[".pdf"],
                        type="filepath"
                    )

                    process_btn = gr.Button(
                        "üìÑ Process PDF",
                        variant="primary",
                        size="lg"
                    )

                    process_output = gr.Textbox(
                        label="Processing Status",
                        lines=3,
                        interactive=False
                    )

                with gr.Column(scale=1):
                    # Q&A Section
                    gr.Markdown("### 2Ô∏è‚É£ Ask Questions")
                    question_input = gr.Textbox(
                        label="Your Question",
                        placeholder="e.g., What is the main concept of machine learning?",
                        lines=2
                    )

                    ask_btn = gr.Button(
                        "ü§ñ Get Answer",
                        variant="secondary",
                        size="lg"
                    )

                    answer_output = gr.Textbox(
                        label="StudyMate's Answer",
                        lines=10,
                        interactive=False
                    )

            # Example questions
            gr.Markdown("""
            ### üí° Example Questions You Can Ask:
            - "What are the key principles mentioned in Chapter 3?"
            - "Explain the methodology described in this paper"
            - "What are the main findings or conclusions?"
            - "Define the term mentioned on page 15"
            - "Summarize the introduction section"

            ### üîß **Troubleshooting Tips:**
            - **If you get timeouts:** Try shorter, more specific questions
            - **For better results:** Ask about specific pages or sections
            - **If API fails:** The system will still show you relevant content from your PDF
            - **Model loading:** Wait 2-3 minutes after first use for optimal performance
            """)

            # Event handlers
            process_btn.click(
                fn=self.process_pdf,
                inputs=[pdf_input],
                outputs=[process_output]
            )

            ask_btn.click(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

            # Allow Enter key to submit question
            question_input.submit(
                fn=self.answer_question,
                inputs=[question_input],
                outputs=[answer_output]
            )

        return interface

# Initialize and launch StudyMate
def launch_studymate():
    """Launch StudyMate application"""
    print("üéì Starting StudyMate - AI-Powered PDF Academic Assistant")
    print("=" * 60)

    # Create StudyMate instance
    studymate = StudyMateApp()

    # Create and launch interface
    interface = studymate.create_interface()

    # Launch with public sharing enabled for Colab
    interface.launch(
        share=True,  # Creates public link for Colab
        debug=True,
        height=800,
        show_error=True
    )

# Run the application
if __name__ == "__main__":
    launch_studymate()